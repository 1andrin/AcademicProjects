{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# ST446 Assignment 2\n--------------------------------------\n\n# P1: Topic modelling on Wikipedia data\n\n#### Download Wikipedia dump file\n\nThis code will download the latest Wikipedia dump into an RDD consisting of string elements, each element corresponding to the text of a Wikipedia page. Precisely, each RDD element will be a string equal to the text enclosed within `<page>` and `</page>` XML tags in the input dataset with carriage-return characters (`\\n`) removed. "}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Wikipedia dump\n# we use the enwiki-latest-pages-articles1.xml-p1p41242.bz2 dump\n#!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2\n# decompress the file\n#!bzip2 -d enwiki-latest-pages-articles1.xml-p1p41242.bz2\n# rename the uncompressed file to an XML format\n#!mv enwiki-latest-pages-articles1.xml-p1p41242 enwiki-latest-pages-articles1.xml\n# move to Hadoop Namenode\n#!hadoop fs -put enwiki-latest-pages-articles1.xml /"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# IMPORTANT: adjust to reflect the cluster name and Hadoop masternode (IP port) of your cluster\npath = \"hdfs://st446-assignment2-cluster-m:8020/enwiki-latest-pages-articles1.xml\""}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;mediawiki xmlns=\"http://www.mediawiki.org/xml...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;Anarchism&lt;/title&gt;\\n    ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanHistory&lt;/tit...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanGeography&lt;/t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanPeople&lt;/titl...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanCommunicatio...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanTransportati...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanMilitary&lt;/ti...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AfghanistanTransnationa...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>\\n  &lt;page&gt;\\n    &lt;title&gt;AssistiveTechnology&lt;/ti...</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                                               value\n0  <mediawiki xmlns=\"http://www.mediawiki.org/xml...\n1  \\n  <page>\\n    <title>Anarchism</title>\\n    ...\n2  \\n  <page>\\n    <title>AfghanistanHistory</tit...\n3  \\n  <page>\\n    <title>AfghanistanGeography</t...\n4  \\n  <page>\\n    <title>AfghanistanPeople</titl...\n5  \\n  <page>\\n    <title>AfghanistanCommunicatio...\n6  \\n  <page>\\n    <title>AfghanistanTransportati...\n7  \\n  <page>\\n    <title>AfghanistanMilitary</ti...\n8  \\n  <page>\\n    <title>AfghanistanTransnationa...\n9  \\n  <page>\\n    <title>AssistiveTechnology</ti..."}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "# initial Dataframe\n# pages are separated by <page> and </page>\ndf1 = spark.read.text(path, lineSep=\"</page>\")\ndf1.limit(10).toPandas()"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "# accessing RDD element attached to the Dataframe\nrdd1 = df1.rdd"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(value='<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\\n  <siteinfo>\\n    <sitename>Wikipedia</sitename>\\n    <dbname>enwiki</dbname>\\n    <base>https://en.wikipedia.org/wiki/Main_Page</base>\\n    <generator>MediaWiki 1.42.0-wmf.22</generator>\\n    <case>first-letter</case>\\n    <namespaces>\\n      <namespace key=\"-2\" case=\"first-letter\">Media</namespace>\\n      <namespace key=\"-1\" case=\"first-letter\">Special</namespace>\\n      <namespace key=\"0\" case=\"first-letter\" />\\n      <namespace key=\"1\" case=\"first-letter\">Talk</namespace>\\n      <namespace key=\"2\" case=\"first-letter\">User</namespace>\\n      <namespace key=\"3\" case=\"first-letter\">User talk</namespace>\\n      <namespace key=\"4\" case=\"first-letter\">Wikipedia</namespace>\\n      <namespace key=\"5\" case=\"first-letter\">Wikipedia talk</namespace>\\n      <namespace key=\"6\" case=\"first-letter\">File</namespace>\\n      <namespace key=\"7\" case=\"first-letter\">File talk</namespace>\\n      <namespace key=\"8\" case=\"first-letter\">MediaWiki</namespace>\\n      <namespace key=\"9\" case=\"first-letter\">MediaWiki talk</namespace>\\n      <namespace key=\"10\" case=\"first-letter\">Template</namespace>\\n      <namespace key=\"11\" case=\"first-letter\">Template talk</namespace>\\n      <namespace key=\"12\" case=\"first-letter\">Help</namespace>\\n      <namespace key=\"13\" case=\"first-letter\">Help talk</namespace>\\n      <namespace key=\"14\" case=\"first-letter\">Category</namespace>\\n      <namespace key=\"15\" case=\"first-letter\">Category talk</namespace>\\n      <namespace key=\"100\" case=\"first-letter\">Portal</namespace>\\n      <namespace key=\"101\" case=\"first-letter\">Portal talk</namespace>\\n      <namespace key=\"118\" case=\"first-letter\">Draft</namespace>\\n      <namespace key=\"119\" case=\"first-letter\">Draft talk</namespace>\\n      <namespace key=\"710\" case=\"first-letter\">TimedText</namespace>\\n      <namespace key=\"711\" case=\"first-letter\">TimedText talk</namespace>\\n      <namespace key=\"828\" case=\"first-letter\">Module</namespace>\\n      <namespace key=\"829\" case=\"first-letter\">Module talk</namespace>\\n    </namespaces>\\n  </siteinfo>\\n  <page>\\n    <title>AccessibleComputing</title>\\n    <ns>0</ns>\\n    <id>10</id>\\n    <redirect title=\"Computer accessibility\" />\\n    <revision>\\n      <id>1002250816</id>\\n      <parentid>854851586</parentid>\\n      <timestamp>2021-01-23T15:15:01Z</timestamp>\\n      <contributor>\\n        <username>Elli</username>\\n        <id>20842734</id>\\n      </contributor>\\n      <minor />\\n      <comment>shel</comment>\\n      <model>wikitext</model>\\n      <format>text/x-wiki</format>\\n      <text bytes=\"111\" xml:space=\"preserve\">#REDIRECT [[Computer accessibility]]\\n\\n{{rcat shell|\\n{{R from move}}\\n{{R from CamelCase}}\\n{{R unprintworthy}}\\n}}</text>\\n      <sha1>kmysdltgexdwkv2xsml3j44jb56dxvn</sha1>\\n    </revision>\\n  ')]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "# first element\nrdd1.take(1)"}, {"cell_type": "markdown", "metadata": {}, "source": "---"}, {"cell_type": "markdown", "metadata": {}, "source": "## P1.1 Creating a document corpus\n\n#### i) Dataframe creation (and pre-processing)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, ArrayType\nfrom pyspark.sql.types import StructType, StructField, StringType, ArrayType\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import col\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\nfrom pyspark.sql.functions import size\nfrom pyspark.ml.feature import CountVectorizer\nimport re"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.appName(\"WikipediaTopicModeling\").getOrCreate()\n\n# Access the existing SparkContext from the SparkSession\nsc = spark.sparkContext\n"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "# Load the Wikipedia dump into an RDD\nwikiDataRDD = rdd1\n"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "#print(wikiDataRDD.take(5))"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "\ndef data_checks(data):\n    # Extract the string from the tuple if necessary\n    text = data[0] if isinstance(data, tuple) else data\n    \n    if not isinstance(text, str):\n        return ''\n    \n    return text\n\ncheckedRDD = wikiDataRDD.map(data_checks)\n"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "#print(checkedRDD.take(5))"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "\ndef parse_page_and_preclean_text(xml):\n    \n    text = xml[0] if isinstance(xml, tuple) else xml\n    \n    if not isinstance(text, str):\n        return None\n        #return \"\"\n    \n    title_match = re.search('<title>(.*?)</title>', xml)\n    title = title_match.group(1) if title_match else None\n\n    id_match = re.search('<id>(.*?)</id>', xml, re.IGNORECASE)\n    page_id = id_match.group(1) if id_match else None\n\n    text_match = re.search('<text.*?>(.*?)</text>', xml, re.DOTALL)\n    text = text_match.group(1) if text_match else None\n\n    \n    if isinstance(text,str):\n        categories = re.findall(r'\\[\\[Category:(.*?)(?:\\|.*)?\\]\\]', text, re.IGNORECASE)\n    else:\n        categories = ['NONE']\n        \n    text = re.sub(r'<!--.*?-->', '', text)\n    text = re.sub(r\"{{.*?}}\", \"\", text)\n    text = re.sub(r\"\\[\\[File:.*?\\]\\]\", \"\", text)\n    text = re.sub(r\"\\[\\[Image:.*?\\]\\]\", \"\", text)\n    text = re.sub(r\"\\n: ''.*?''\", \"\", text)\n    text = re.sub(r\"\\n!.*\", \"\", text)\n    text = re.sub(r\"^:''.*\", \"\", text)\n    text = re.sub(r\"&nbsp\", \"\", text)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"\\d+\", \"\", text)\n    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n    text = re.sub(r\"Category:.*\", \"\", text)\n    text = re.sub(r\"\\| .*\", \"\", text)\n    text = re.sub(r\"\\n\\|.*\", \"\", text)\n    text = re.sub(r\"\\n \\|.*\", \"\", text)\n    text = re.sub(r\".* \\|\\n\", \"\", text)\n    text = re.sub(r\".*\\|\\n\", \"\", text)\n    text = re.sub(r\"{{Infobox.*\", \"\", text)\n    text = re.sub(r\"{{infobox.*\", \"\", text)\n    text = re.sub(r\"{{taxobox.*\", \"\", text)\n    text = re.sub(r\"{{Taxobox.*\", \"\", text)\n    text = re.sub(r\"{{ Infobox.*\", \"\", text)\n    text = re.sub(r\"{{ infobox.*\", \"\", text)\n    text = re.sub(r\"{{ taxobox.*\", \"\", text)\n    text = re.sub(r\"{{ Taxobox.*\", \"\", text)\n    text = re.sub(r\"\\* .*\", \"\", text)\n    text = re.sub(r\"<.*?>\", \"\", text)\n    text = re.sub(r\"\\n\", \"\", text)\n    text = re.sub(r\"\\!|\\\"|\\#|\\$|\\%|\\&|\\'|\\(|\\)|\\*|\\+|\\,|\\-|\\.|\\/|\\:|\\;|\\|\\?|\\@|\\[|\\\\|\\]|\\^|\\_|\\`|\\{|\\||\\}|\\~\", \" \", text)\n    text = re.sub(r\" +\", \" \", text)\n    text = re.sub(r'== See also ==.*', '', text, flags=re.DOTALL)\n    text = re.sub(r'<ref>.*?</ref>', '', text)\n    text = re.sub(r'&lt;|&gt;|&quot;|&amp;', '', text)\n    text = re.sub(r'\\[\\[Category:.*?\\]\\]', '', text)\n    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n    text = re.sub(r'<ref>.*?</ref>', '', text, flags=re.DOTALL)\n    text = re.sub(r'\\[http[^\\]]*\\]', '', text)\n    text = re.sub(r'\\n+', ' ', text)\n    text = re.sub(r\"'{2,}\", '', text) # Remove bold/italic markup\n    text = re.sub(r\"\\[\\[(?:[^\\]|]*\\|)?([^\\]]+)\\]\\]\", r'\\1', text) \n    text = re.sub(r\"\\s{2,}|\\n\", \" \", text).strip()\n        \n    # Limit text to first 3000 words\n    text = \" \".join(text.split()[:3000]) if text else None\n    \n    return Row(title=title, page_id=page_id, text=text, categories=categories)\n\n# Apply the parsing function to each element in the RDD\nparsedRDD_raw = checkedRDD.map(parse_page_and_preclean_text).filter(lambda x: x is not None)\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "As wikipedia pages are quite difficult to clean, the list of predefined regular expressions is used: [Topic Modelling Part 1: Creating Article Corpus from Simple Wikipedia Dump by Abhijeet Kumar](https://appliedmachinelearning.wordpress.com/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/)\n"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "pagesWithCategoriesRDD = parsedRDD_raw.filter(lambda row: len(row.categories) > 0)\n"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "schema = StructType([\n    StructField(\"title\", StringType(), True),\n    StructField(\"page_id\", StringType(), True),  \n    StructField(\"text\", StringType(), True),\n    StructField(\"categories\", ArrayType(StringType()), True)\n])\n\ncorpusDF = spark.createDataFrame(pagesWithCategoriesRDD, schema=schema)"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "corpusDF = corpusDF.filter(\n    col(\"title\").isNotNull() & \n    (col(\"text\") != \"\")\n)\n\ncorpusDF = corpusDF.filter((col(\"categories\").isNotNull()) & (size(col(\"categories\")) > 0))\n"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import split\n\ncorpusDF = corpusDF.filter(size(split(col(\"text\"), \" \")) > 10)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### ii) Top 20 rows of the corpusDF Dataframe"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+--------------------+--------------------+--------------------+\n|page_id|               title|                text|          categories|\n+-------+--------------------+--------------------+--------------------+\n|     12|           Anarchism|Anarchism is a po...|[Anarchism, Anti-...|\n|     39|              Albedo|The map shows the...|[Land surface eff...|\n|    290|                   A|A sharp A or a is...|[ISO basic Latin ...|\n|    303|             Alabama|Alabama lt ref gt...|[Alabama, 1819 es...|\n|    305|            Achilles|In Greek mytholog...|[Greek mythologic...|\n|    307|     Abraham Lincoln|Abraham Lincoln w...|[Abraham Lincoln,...|\n|    308|           Aristotle|collapsible list ...|[Aristotle, Arist...|\n|    309|An American in Paris|An American in Pa...|[1928 composition...|\n|    316|Academy Award for...|The Academy Award...|[Academy Awards, ...|\n|    324|      Academy Awards|The Academy Award...|[Academy Awards, ...|\n|    330|             Actrius|Actresses is a Ca...|[1997 films, 1997...|\n|    332|     Animalia (book)|lt gt Animalia is...|[1986 children's ...|\n|    334|International Ato...|International Ato...|       [Time scales]|\n|    336|            Altruism|to the poor is of...|[Altruism, August...|\n|    339|            Ayn Rand|Alice O Connor be...|[Ayn Rand, 1905 b...|\n|    340|        Alain Connes|Alain Connes is a...|[1947 births, Liv...|\n|    344|          Allan Dwan|Allan Dwan was a ...|[1885 births, 198...|\n|    358|             Algeria|Algeria officiall...|[Algeria, North A...|\n|    359|List of Atlas Shr...|This is a list of...|[Atlas Shrugged c...|\n|    569|        Anthropology|Anthropology is t...|[Anthropology, Be...|\n+-------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"}], "source": "\n# Filter out rows where 'text' column starts with \"REDIRECT\" or contains \"disambiguation}}\"\ncorpusDF = corpusDF.filter(~col(\"text\").startswith(\"REDIRECT\") & ~col(\"text\").contains(\"disambiguation}}\"))\n\n\n# Show the cleaned DataFrame to verify the results\ncorpusDF.select(\"page_id\", \"title\", \"text\", \"categories\").show(20)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### iii) Applying pre-processing steps to parse the data, removing stop words and any other special symbols, and creating feature vectors and vocabulary"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\nimport string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\n\n# Assuming necessary NLTK data is downloaded\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"Wikipedia Processing\").getOrCreate()\n\n# Define the custom processing function\nadditional_stop_words = {'lt', 'gt', 'ref', 'quot', 'amp', 'nbsp', 'p', 'pp'}\nstop_words = set(nltk.corpus.stopwords.words('english')).union(additional_stop_words)\ntable = str.maketrans('', '', string.punctuation)\nlmtzr = WordNetLemmatizer()\n\ndef custom_processing(words):\n    cleaned_tokens = [w.lower().translate(table) for w in words]\n    filtered_tokens = [w for w in cleaned_tokens if w.isalpha() and w not in stop_words]\n    lemmatized_tokens = [lmtzr.lemmatize(w) for w in filtered_tokens]\n    return lemmatized_tokens\n\n# Register the UDF\ncustom_processing_udf = udf(custom_processing, ArrayType(StringType()))\n\n# Tokenize the text\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\ntokenized_df = tokenizer.transform(corpusDF)\n\n# Apply the custom UDF to process tokens\nprocessed_tokens_df = tokenized_df.withColumn(\"processed_tokens\", custom_processing_udf(col(\"tokens\")))\n\n"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import col, when\nfrom pyspark.sql.functions import array\n\n# it does not work performance wise with the full data unfortunately. I have not been able to find the mistake but tried several settings for minDF etc.\n#processed_tokens_sample = processed_tokens_df.sample(False, 0.1) # also does not finish processing (1h)\nprocessed_tokens_sample = processed_tokens_df.limit(20) # works\n\n# Replace null values in 'filtered_words' with an empty list (assuming it's a list of tokens)\nprocessed_tokens_df = processed_tokens_sample.withColumn(\"processed_tokens\", when(col(\"processed_tokens\").isNull(), array()).otherwise(col(\"processed_tokens\")))\n\n"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "\n# Setup CountVectorizer\ncv = CountVectorizer(inputCol=\"processed_tokens\", outputCol=\"features\", minDF=10)\n\n# Fit and transform the data\ncv_model = cv.fit(processed_tokens_sample)\nresult_df = cv_model.transform(processed_tokens_sample)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### iv) Showing the first 20 feature vectors and corresponding vocabulary entries"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+--------------------+\n|page_id|            features|\n+-------+--------------------+\n|     12|(64,[1,2,3,4,6,7,...|\n|     39|(64,[0,2,4,6,7,8,...|\n|    290|(64,[0,2,3,4,6,7,...|\n|    303|(64,[0,1,2,3,4,5,...|\n|    305|(64,[0,1,2,3,4,6,...|\n|    307|(64,[0,1,2,3,4,5,...|\n|    308|(64,[0,1,2,3,4,6,...|\n|    309|(64,[0,1,2,3,4,5,...|\n|    316|(64,[0,3,9,10,17,...|\n|    324|(64,[0,1,2,3,4,5,...|\n|    330|(64,[0,2,3,5,7,13...|\n|    332|(64,[4,5,7,8,9,10...|\n|    334|(64,[0,1,2,3,4,6,...|\n|    336|(64,[0,1,2,3,4,6,...|\n|    339|(64,[0,1,2,3,4,5,...|\n|    340|(64,[0,1,3,5,7,13...|\n|    344|(64,[0,1,2,3,4,5,...|\n|    358|(64,[0,1,2,3,4,6,...|\n|    359|(64,[0,1,2,3,4,5,...|\n|    569|(64,[0,1,2,3,4,5,...|\n+-------+--------------------+\n\n"}], "source": "# Show the results\nresult_df.select(\"page_id\", \"features\").show(20)"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Vocabulary: ['name', 'state', 'time', 'first', 'one', 'american', 'form', 'also', 'new', 'year', 'th', 'century', 'people', 'work', 'may', 'united', 'many', 'science', 'used', 'study']\n"}], "source": "\n# Show the vocabulary\nvocabulary = cv_model.vocabulary\nprint(\"Vocabulary:\", vocabulary[:20])"}, {"cell_type": "markdown", "metadata": {}, "source": "---"}, {"cell_type": "markdown", "metadata": {}, "source": "## P1.2 Perform topic modelling"}, {"cell_type": "markdown", "metadata": {}, "source": "#### i) Identifying 10 topics across all Wikipedia pages and no less than 6 words describing each topic. Showing the top 6 words and corresponding weights for each topic "}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Topic 0:\n    form: 0.13405627798106887\n    time: 0.06764501697683707\n    used: 0.041258257564561036\n    science: 0.030883014775732326\n    one: 0.030689576588536077\n    many: 0.03004463638564091\n\n\nTopic 1:\n    including: 0.019699047808233176\n    new: 0.018335378979625106\n    word: 0.018203020804433662\n    member: 0.018019298726935938\n    two: 0.017866489950531896\n    several: 0.017751519613775503\n\n\nTopic 2:\n    modern: 0.01957354860870912\n    year: 0.01942293264302994\n    known: 0.019068745190909484\n    became: 0.018838649078161875\n    long: 0.018099257566906803\n    among: 0.017707667044135084\n\n\nTopic 3:\n    american: 0.020103015804952693\n    since: 0.01932282976899238\n    th: 0.018929940160454562\n    day: 0.018504321269705526\n    many: 0.017794612510482158\n    world: 0.017579242013252862\n\n\nTopic 4:\n    well: 0.02002803206411524\n    link: 0.018451846775392723\n    used: 0.018279177154636127\n    second: 0.018070472440232715\n    origin: 0.01796578771803066\n    since: 0.017642693054614775\n\n\nTopic 5:\n    name: 0.15359248302948866\n    state: 0.11975290659657142\n    american: 0.04727205479912834\n    first: 0.039641942303878096\n    time: 0.03719623559188425\n    united: 0.03230296889194297\n\n\nTopic 6:\n    although: 0.01983317583810657\n    known: 0.01926383576634232\n    became: 0.0188077146054572\n    also: 0.01859649515470913\n    end: 0.01828196244562884\n    began: 0.017653293287895354\n\n\nTopic 7:\n    day: 0.018991590052268678\n    man: 0.0189788003614568\n    time: 0.01874089005404203\n    since: 0.01865184541644671\n    around: 0.018530597503061734\n    used: 0.01786631130328459\n\n\nTopic 8:\n    made: 0.019394992907405567\n    became: 0.01830470851670424\n    three: 0.018300307148780996\n    two: 0.018230823469431424\n    among: 0.017954102592521948\n    long: 0.017410310175083696\n\n\nTopic 9:\n    among: 0.0207698128586592\n    found: 0.019089965661054864\n    well: 0.018592786233892607\n    part: 0.01799879070176472\n    science: 0.017840035219504963\n    people: 0.017727764400552713\n\n\n"}], "source": "from pyspark.ml.clustering import LDA\n\n# Specify the number of topics\nnum_topics = 10\n\n# Initialize LDA with the desired number of topics\nlda = LDA(k=num_topics, maxIter=10)\n\n# Fit the LDA model to your data\nlda_model = lda.fit(result_df)\n\n# Extract topic distributions\ntopics = lda_model.describeTopics(maxTermsPerTopic=6)\n\n# Show the topics with words\nvocab_list = cv_model.vocabulary\ntopics_rdd = topics.rdd\n\n# Map topic indices to words\ntopics_words = topics_rdd\\\n    .map(lambda row: (row[0], list(zip([vocab_list[word_idx] for word_idx in row[1]], row[2]))))\\\n    .collect()\n\n# Print topics\nfor topic in topics_words:\n    print(f\"Topic {topic[0]}:\")\n    for word, weight in topic[1]:\n        print(f\"    {word}: {weight}\")\n    print(\"\\n\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### ii) Printing title, topic and corresponding words for the first 10 pages in the Dataframe"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Title: Anarchism\nDominant Topic: 0\nWords: form: 0.1341, time: 0.0676, used: 0.0413, science: 0.0309, one: 0.0307, many: 0.0300\n\n\nTitle: Albedo\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\nTitle: A\nDominant Topic: 0\nWords: form: 0.1341, time: 0.0676, used: 0.0413, science: 0.0309, one: 0.0307, many: 0.0300\n\n\nTitle: Alabama\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\nTitle: Achilles\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\nTitle: Abraham Lincoln\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\nTitle: Aristotle\nDominant Topic: 0\nWords: form: 0.1341, time: 0.0676, used: 0.0413, science: 0.0309, one: 0.0307, many: 0.0300\n\n\nTitle: An American in Paris\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\nTitle: Academy Award for Best Production Design\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\nTitle: Academy Awards\nDominant Topic: 5\nWords: name: 0.1536, state: 0.1198, american: 0.0473, first: 0.0396, time: 0.0372, united: 0.0323\n\n\n"}], "source": "\n# Transform the vectorized DataFrame to get topic distributions\ntransformed_df = lda_model.transform(result_df)\n\n# Select the first 10 rows, titles, and topic distribution\nfirst_10_pages = transformed_df.select(\"title\", \"topicDistribution\").take(10)\n\n# Get the topic words from the LDA model\ntopics_words = lda_model.describeTopics(maxTermsPerTopic=6)\nvocab_list = cv_model.vocabulary\ntopics_words_rdd = topics_words.rdd\ntopics_descriptions = topics_words_rdd\\\n    .map(lambda row: (row[0], list(zip([vocab_list[word_idx] for word_idx in row[1]], row[2]))))\\\n    .collect()\n\n# Dictionary to map topic index to words\ntopics_dict = {topic[0]: topic[1] for topic in topics_descriptions}\n\nfor page in first_10_pages:\n    title = page['title']\n    # Get the index of the highest topic weight\n    dominant_topic_index = page['topicDistribution'].argmax()\n    # Get the words for the dominant topic\n    topic_words = topics_dict[dominant_topic_index]\n    # Print the information\n    print(f\"Title: {title}\")\n    print(f\"Dominant Topic: {dominant_topic_index}\")\n    print(\"Words:\", \", \".join([f\"{word}: {weight:.4f}\" for word, weight in topic_words]))\n    print(\"\\n\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### iii) Analyse and discuss whether the topic and words actually represent (or approximate) the title and categories of each page."}, {"cell_type": "markdown", "metadata": {}, "source": "#### Conclusion\nWhile the LDA model provides an interesting overview of the thematic structure within the Wikipedia dataset, the analysis reveals that the topics and their corresponding words might not always represent or accurately approximate the specific title and categories of each page. This suggests a need for further refinement of the topic modeling process, such as adjusting the number of topics, incorporating more nuanced text processing, or applying domain-specific adjustments to better capture the essence of each Wikipedia page.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# P2: Big data programming (PDF version in GitHub Repo)\n\n## P2.1 MapReduce programming"}, {"cell_type": "markdown", "metadata": {}, "source": "#### i) Provide a code snippet for a `map()` and `reduce()` functions showing how you would determine the maximum temperature each year, along with a brief explanation of your code."}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": "# Define the map function\ndef map_temperature(record):\n    date, temperature = record.split('; ')\n    year = date.split('-')[0]\n    return (year, int(temperature))"}, {"cell_type": "markdown", "metadata": {}, "source": "The `map()` function processes each line of the dataset, extracting the year from the date and emitting a key-value pair where the key is the year and the value is the temperature."}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": "# Define the reduce function to find the maximum temperature for each year\ndef reduce_max_temperature(group):\n    year, temperatures = group\n    max_temperature = max(temperatures, key=lambda x: x[1])[1]\n    return (year, max_temperature)"}, {"cell_type": "markdown", "metadata": {}, "source": "The `reduce()` function takes key-value pairs produced by the map() function, groups them by key (year), and then reduces each group to a single key-value pair where the value is the maximum temperature recorded in that year."}, {"cell_type": "markdown", "metadata": {}, "source": "**Explanation:**\n- The map() function is applied to each record in the dataset. It extracts the year from the date and emits a key-value pair with the year as the key and the observed temperature as the value.\n\n- The framework groups all key-value pairs by key (year), resulting in an intermediate dataset where each key (year) is associated with a list of temperatures recorded in that year.\n\n- The reduce() function is then applied to each group (year and its corresponding list of temperatures). It calculates the maximum temperature for that year and emits a key-value pair with the year as the key and the maximum temperature as the value.\n\n- The result is a dataset where each record represents a year and the highest temperature recorded in that year.\n\nThis process allows for efficient processing of large datasets by distributing the computation across multiple nodes in a cluster, where each node can process a subset of the data in parallel."}, {"cell_type": "markdown", "metadata": {}, "source": "**Example:**"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"data": {"text/plain": "[('2015', 30), ('2016', 27), ('2017', 33)]"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "# Sample temperature data representing (date; temperature) format\ndata = [\n    \"2015-07-13; 24\",\n    \"2015-08-15; 30\",\n    \"2015-12-02; 20\",\n    \"2016-07-12; 25\",\n    \"2016-08-19; 27\",\n    \"2016-02-02; 19\",\n    \"2017-07-01; 28\",\n    \"2017-08-22; 33\",\n    \"2017-12-11; 22\"\n]\n\n# Apply the map function to each record\nmapped_temperatures = map(map_temperature, data)\n\n# Sort and group by year to prepare for reduction\nfrom itertools import groupby\nmapped_temperatures = sorted(mapped_temperatures, key=lambda x: x[0])\ngrouped_temperatures = groupby(mapped_temperatures, key=lambda x: x[0])\n\n\n# Apply the reduce function\nmax_temperatures_by_year = [reduce_max_temperature(group) for group in grouped_temperatures]\n\nmax_temperatures_by_year"}, {"cell_type": "markdown", "metadata": {}, "source": "#### ii) Explain how the input keys of the map phase will influence data movement across the network."}, {"cell_type": "markdown", "metadata": {}, "source": "Understanding the influence of input keys in the map phase on data movement across the network is critical in optimizing the performance of MapReduce jobs. When data is processed in a distributed system using MapReduce, the goal is to minimize the amount of data transmitted over the network because network I/O can significantly slow down the computation, especially when dealing with large datasets.\n\n#### The Influence of Input Keys on Data Movement\n\nDuring the map phase, the input dataset is split into smaller chunks, which are then processed in parallel across different nodes in the cluster. The output of the map phase is a set of intermediate key-value pairs. How these keys are chosen and partitioned will significantly impact the subsequent shuffle and sort phase, where the intermediate data is grouped (shuffled) by key and transferred across the network to the reducers.\n\nIf the map phase produces a large number of unique keys, this can lead to a substantial amount of data movement during the shuffle phase, as the system needs to transfer these key-value pairs across the network to ensure that all values associated with the same key are brought to the same reducer for processing. On the other hand, if the keys are well-distributed and the amount of data per key is balanced, the network traffic can be minimized, leading to more efficient processing.\n\n#### Strategies to Minimize Data Movement\n\n1. **Key Choice**: Choosing the right keys is crucial. Keys should be selected in a way that they evenly distribute the data across reducers, avoiding scenarios where a single reducer is overwhelmed with a large portion of the data.\n2. **Combiner Function**: A combiner function can be used locally on each mapper to perform a preliminary reduction of the data, which can significantly reduce the amount of data that needs to be sent over the network.\n3. **Partitioning**: Custom partitioners can be implemented to control how map output keys are assigned to reducers, ensuring a more even distribution of the data load.\n\n**Conclusion**\n\nThe choice of input keys in the map phase and how they are handled can dramatically affect the efficiency of a MapReduce job by influencing the amount of data transferred over the network. Efficient data processing in distributed systems requires careful consideration of how data is partitioned and aggregated to minimize costly network transfers and ensure balanced workloads across the cluster.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## P2.2 Machine learning pipeline"}, {"cell_type": "markdown", "metadata": {}, "source": "#### i) Would combining different machine learning models in a Spark pipeline make sense to perform multiclass classification? Justify your answer."}, {"cell_type": "markdown", "metadata": {}, "source": "Combining different machine learning models in a Spark pipeline to perform multiclass classification can indeed make sense and offer several advantages, depending on the complexity of the task, the diversity of the data, and the specific goals of the classification problem. This approach is often referred to as ensemble learning, where multiple models are used together to improve the overall performance. Here are some reasons why this approach can be beneficial:\n\n1. **Improved Accuracy**: Different models may have different strengths and weaknesses depending on the characteristics of the data. By combining these models, it's possible to leverage their strengths and mitigate their weaknesses, leading to improved overall accuracy in the classification task.\n\n2. **Robustness to Overfitting**: Individual models might overfit to the training data, especially if the model is too complex or the data is noisy. An ensemble of models, especially if they are diverse (e.g., decision trees, logistic regression, support vector machines), can reduce the risk of overfitting by averaging out their predictions, making the final decision more robust.\n\n3. **Handling Data Variety**: Multiclass classification problems often involve datasets with a wide variety of features and data types. Some models might be better at handling certain types of data or features. Combining models allows for a more flexible approach to dealing with different data characteristics.\n\n4. **Increased Model Complexity**: Some problems might be too complex for a single model to capture all the nuances in the data. A combination of models can express a more complex decision boundary than any individual model, potentially leading to better performance on complex problems.\n\n5. **Spark\u2019s Distributed Computing**: Spark\u2019s ability to distribute computation across a cluster makes it an ideal platform for implementing such combined models. Training multiple models in parallel and aggregating their predictions can be efficiently performed in a distributed manner, making the process scalable and faster compared to single-model approaches on large datasets.\n\n#### Implementation Considerations\n\nWhen implementing a combination of models in a Spark pipeline, it's important to consider:\n\n- **Model Selection and Diversity**: The choice of models to combine should be based on their ability to complement each other. Too similar models might not provide the benefits expected from an ensemble approach.\n- **Combination Strategy**: Strategies like voting, averaging, or more complex methods like stacking can be used to combine the predictions from different models. The choice depends on the problem and the models used.\n- **Resource and Time Constraints**: Training multiple models and combining their predictions require more computational resources and time. It's crucial to balance the performance gains with the additional costs.\n\n#### Downsides of Combining Different Machine Learning Models in a Spark Pipeline\n\nWhile combining different machine learning models in a Spark pipeline for multiclass classification offers numerous benefits, it also comes with its own set of challenges and downsides. Here are some potential drawbacks to consider:\n\n1. **Increased Complexity**: Managing multiple models within a pipeline can significantly increase the complexity of the system. This complexity arises not only from the need to tune multiple models but also from the complexity of combining their predictions in a meaningful way.\n\n2. **Higher Resource Demand**: Training and deploying multiple models require more computational resources. This can lead to increased costs and longer training times, especially for large datasets. While Spark efficiently distributes these tasks, the overhead of coordinating multiple models can still be significant.\n\n3. **Difficulty in Interpretation**: One of the trade-offs for improved performance can be the loss of interpretability. Single models, especially simpler ones like decision trees or linear models, can offer clear insights into how predictions are made. Combining models, especially using complex strategies like stacking, can make it harder to interpret how the final predictions are derived.\n\n4. **Model Tuning and Selection**: Finding the right combination of models and tuning their hyperparameters can be a daunting task. The search space for the optimal setup grows exponentially with the addition of each model, making the tuning process more challenging and time-consuming.\n\n5. **Risk of Diminishing Returns**: There's a point of diminishing returns where adding more models to the ensemble does not significantly improve performance and might even degrade it due to overfitting or increased variance in the combined predictions.\n\n6. **Dependency and Failure Risk**: Relying on multiple models means dealing with the dependencies and potential failures of more components. If the performance of one model in the ensemble significantly degrades, it can negatively impact the overall system's reliability and accuracy.\n\n### Balancing Act\n\nIt's essential to carefully consider these downsides when deciding to combine multiple machine learning models in a Spark pipeline. The decision should be based on a thorough analysis of the problem complexity, the available resources, the importance of interpretability, and the specific requirements of the application. In many cases, the benefits of improved accuracy and robustness may outweigh the downsides, especially for complex, high-stakes classification tasks. However, for simpler problems or when resources are limited, a well-tuned single model might be a more efficient and interpretable choice.\n\n\n#### Conclusion: Weighing the Pros and Cons of Combining Models in Spark Pipelines\n\nCombining models in spark pipelines has many advantages balanced by some downsides. The primary benefits of such an approach include enhanced predictive performance, robustness against overfitting, and the ability to capture complex patterns and relationships in the data that a single model might miss. These benefits can be particularly valuable in scenarios where the highest possible accuracy is crucial, and the complexity of the data exceeds the capacity of individual models.\n\nOn the other hand, the increased system complexity, higher resource demands, potential interpretability issues, and the complexities involved in model tuning and selection represent challenges that need careful consideration. The key to leveraging the advantages while mitigating the downsides lies in a strategic and informed approach to model selection and integration within the pipeline. This involves:\n\n- **Thorough Evaluation**: Systematically testing different combinations and configurations to find the optimal balance between performance and complexity.\n- **Resource Management**: Leveraging Spark's distributed computing capabilities to manage resource demands efficiently.\n- **Interpretability Strategies**: Employing techniques like feature importance metrics and model-agnostic interpretation tools to maintain insight into how predictions are made.\n- **Regular Monitoring**: Continuously monitoring model performance to quickly identify and address any issues arising from model interactions or external changes in data patterns.\n\nUltimately, while the decision to combine models introduces complexity and challenges, the potential for significant performance gains makes it a compelling strategy for complex classification tasks. By carefully planning and managing the pipeline, it is possible to harness the strengths of multiple models to achieve superior results, making the most of the scalable and powerful environment that Spark provides for big data and machine learning applications.\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### ii) How would you structure such a pipeline in terms of Transformers and Estimators? Provide a code snippet for your solution and briefly explain it."}, {"cell_type": "markdown", "metadata": {}, "source": "When dealing with multiclass classification in Spark, it often makes sense to explore different machine learning models to find the best approach for a given dataset. However, rather than relying on a single model, combining multiple models can lead to better performance by leveraging their individual strengths and mitigating their weaknesses. This is where a machine learning pipeline that incorporates multiple models becomes valuable.\n\n#### Goal and Problem\nThe goal is to create a robust multiclass classification system that can accurately classify instances into one of several categories. The problem arises from the fact that different models have different biases and variances, and what works well for one part of the data might not work as well for another. By combining models, we can potentially create a system that performs better on average across the entire dataset.\n\n#### How to Structure the Pipeline\nIn terms of structuring such a pipeline, it involves a series of steps where data is preprocessed (e.g., normalized, tokenized), fed into different models, and then the results of these models are combined to produce a final classification. This process typically involves Transformers and Estimators in Spark:\n\n1. **Transformers** for preprocessing data. These can include feature vectorizers, normalizers, and any other data transformation steps necessary before model training.\n2. **Estimators** for model training. These are the machine learning algorithms you want to train, such as logistic regression, random forest, or gradient-boosted trees.\n3. A custom **Transformer** or strategy to combine the outputs of your models. This could be as simple as taking the mode of predictions from all models (voting) or more complex strategies like stacking or blending.\n\nSince Spark ML's Pipeline API is designed to handle linear workflows (one set of transformations followed by a single estimator), incorporating multiple models directly into a single pipeline for training requires a custom approach. After training, the models' predictions can be combined using a custom Transformer or by manually implementing a combination strategy outside of the Pipeline.\n\n#### Example Code and Explanation\nBelow is a simplified example that outlines the main components of such a pipeline:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Initialize a Spark session\nspark = SparkSession.builder.appName(\"MulticlassClassificationPipeline\").getOrCreate()\n\n# Example DataFrame with features and labels\ndata = spark.createDataFrame([\n    (0, \"text data example 1\", 1.0),\n    (1, \"text data example 2\", 0.0),\n    (2, \"text data example 3\", 1.0)\n], [\"id\", \"text\", \"label\"])\n\n# Data preprocessing (Transformers)\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\nvector_assembler = VectorAssembler(inputCols=data.columns[:-1], outputCol=\"features\")\ndata = tokenizer.transform(data)\ndata = hashingTF.transform(data)\ndata = vector_assembler.transform(data)\n\n# Split data into train and test sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n\n\n# Models (Estimators)\nlr = LogisticRegression(maxIter=10, featuresCol=\"features\", labelCol=\"label\")\nrf = RandomForestClassifier(numTrees=10, featuresCol=\"features\", labelCol=\"label\")\ngbt = GBTClassifier(maxIter=10, featuresCol=\"features\", labelCol=\"label\")\n\n# Fit the model\nlr_model = lr.fit(train_data)\nrf_model = lr.fit(train_data)\n\n# Make predictions\npredictions_lr = lr_model.transform(test_data)\npredictions_rf = rf_model.transform(test_data)\n\n# Simulated pipeline steps (for illustration)\n\n# Combining model outputs via voting (a conceptual Transformer)\nclass VotingTransformer(Transformer):\n    def __init__(self):\n        super(VotingTransformer, self).__init__()\n    \n    def _transform(self, dataset):\n        # Assuming 'predictions_lr' and 'predictions_rf' are columns containing model predictions\n        # Create a column 'final_prediction' based on simple majority voting\n        dataset = dataset.withColumn('final_prediction', \n                                     when(col('predictions_lr') == col('predictions_rf'), col('predictions_lr'))\n                                     .otherwise(when(col('predictions_lr') != col('predictions_rf'), \n                                                     F.lit('Tie'))  # Example handling for ties\n                                                )\n                                    )\n        return dataset\n\n# Example usage within a conceptual pipeline\npipeline_stages = [tokenizer, hashingTF, lr, rf, VotingTransformer()]\npipeline = Pipeline(stages=pipeline_stages)\n\n# This example demonstrates where Transformers and Estimators are in the pipeline,\n# with the final step being a custom Transformer to combine predictions."}, {"cell_type": "markdown", "metadata": {}, "source": "In this conceptual example, we illustrate how a machine learning pipeline for multiclass classification could be structured within Apache Spark's ML library. The pipeline integrates multiple models and employs a strategy for combining their predictions to enhance classification performance.\n\n#### Explanation:\n\n1. **Data Preparation**: We start by preparing the text data using Transformers like `Tokenizer` and `HashingTF` to convert text into feature vectors. Additionally, a `VectorAssembler` is used to assemble these features into a single vector.\n\n2. **Model Training**: We define three different classifiers: Logistic Regression, Random Forest, and Gradient-Boosted Trees. Each of these models serves as an Estimator in the pipeline, meaning they are responsible for learning from the training data.\n\n3. **Combining Model Outputs**: The most distinctive part of this pipeline is the custom `VotingTransformer`. This conceptual Transformer takes the predictions from the logistic regression and random forest models and combines them through a simple majority voting mechanism. For simplicity, ties are marked as 'Tie', but in practice, more sophisticated tie-breaking strategies or outputting probabilities could be employed.\n\n#### Conclusion:\n\nBy leveraging Spark's Pipeline API, we can represent the entire process of data preprocessing, model training, and prediction aggregation in a coherent and scalable manner. This conceptual example demonstrates the flexibility of Spark's machine learning library in accommodating complex workflows such as integrating multiple models. While each model individually contributes to the classification task, their combination through a voting mechanism potentially improves the robustness and accuracy of the final predictions."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 5}